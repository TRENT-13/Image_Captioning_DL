{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24108ff1",
      "metadata": {
        "id": "24108ff1"
      },
      "source": [
        "# Idea\n",
        "    at first we want to extract a features and based on that  get the info\n",
        "\n",
        "First, which model should we choose for the Image pretrining?\n",
        "if you ask LLM's VGG is no brainer, thats because they have feed too many github repos which uses VGG to solve this problem, but in reality we can far better and newer models, as for me , i am choosing efficentNets, smaller lower FLOP's, lower resources better results,![image.png](attachment:image.png),"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9a346e",
      "metadata": {
        "id": "9a9a346e"
      },
      "source": [
        "i am using collab, assume you have the zip file on collab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4980aa17",
      "metadata": {
        "id": "4980aa17"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90e25a1",
      "metadata": {
        "id": "e90e25a1"
      },
      "outputs": [],
      "source": [
        "# Change to the directory where your zip file is located (e.g., My Drive)\n",
        "%cd /content/drive/My Drive/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you already have the unzipped data on your google drive, you can avoid running the next cell. Just make sure that the directory paths are correct in the config."
      ],
      "metadata": {
        "id": "-a_PBVx5ObyY"
      },
      "id": "-a_PBVx5ObyY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e816f8ff",
      "metadata": {
        "id": "e816f8ff"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "!unzip caption_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4520b743",
      "metadata": {
        "id": "4520b743"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42) # this should be automatically called by torch.manual_seed(), but just to make sure\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5215e804",
      "metadata": {
        "id": "5215e804"
      },
      "outputs": [],
      "source": [
        "# could be modified\n",
        "class Config:\n",
        "    # Paths\n",
        "    IMAGE_DIR = \"caption_data/Images\"\n",
        "    CAPTIONS_FILE = \"caption_data/captions.txt\"\n",
        "    FEATURES_PATH = \"caption_data/resnet_features.pt\"\n",
        "    VOCAB_PATH = \"vocab.pkl\"\n",
        "\n",
        "    # Model\n",
        "    EMBED_SIZE = 256\n",
        "    HIDDEN_SIZE = 512\n",
        "    NUM_LAYERS = 1\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    NUM_WORKERS = 2 if torch.cuda.is_available else 0\n",
        "\n",
        "    # Training\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 3e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    GRAD_CLIP = 5.0\n",
        "\n",
        "    # Data\n",
        "    FREQ_THRESHOLD = 5  # could be modified\n",
        "    MAX_SEQ_LEN = 50 # I don't think we need to use this\n",
        "\n",
        "    # Splits\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1e808d",
      "metadata": {
        "id": "8e1e808d"
      },
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize((260, 260)),\n",
        "    # T.RandomCrop((224, 224)), we can do that, but what if it crops the small neededed object\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    # T.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)), too brutal\n",
        "    T.RandomGrayscale(p=0.02), # we can delete it because the color is important for us, we do not grey sky output\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # efficent Net is using Imagenet data for the training so using these values here wont be a data leakage because these are the values of the Imagenet, even though our data could be different this values work well\n",
        "])\n",
        "\n",
        "# no shenanigans for this one\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((260, 260)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e47fbe92",
      "metadata": {
        "id": "e47fbe92"
      },
      "outputs": [],
      "source": [
        "class EfficientNetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    EfficientNet-B2 feature extractor - COMPLETELY FROZEN\n",
        "    Only the projection layer is trainable\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained EfficientNet-B2\n",
        "        effnet = models.efficientnet_b2(\n",
        "            weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
        "        )\n",
        "\n",
        "        # EfficientNet structure:\n",
        "        # features -> avgpool -> classifier\n",
        "        self.backbone = effnet.features\n",
        "        self.avgpool = effnet.avgpool\n",
        "\n",
        "        # Feature dimension for B2\n",
        "        backbone_out = 1408\n",
        "\n",
        "        # Freeze backbone\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Projection head (trainable)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(backbone_out, embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            x = self.backbone(images)\n",
        "            x = self.avgpool(x)\n",
        "\n",
        "        features = self.projection(x)\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c2aaf5-ba2f-4276-bba2-8a0583d1d8a4",
      "metadata": {
        "id": "48c2aaf5-ba2f-4276-bba2-8a0583d1d8a4"
      },
      "source": [
        "## Vocabulary, Dataset and DataLoader\n",
        "The next step is to write the Vocabulary logic that assigns unique integers to words and a dataloader that loads an image and its captions, preprocesses them and hands them to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a870d874-05cc-4991-963b-61784dd834b1",
      "metadata": {
        "id": "a870d874-05cc-4991-963b-61784dd834b1"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"} # integers to tokens for specific token values\n",
        "    # make sure that padding index is 0. it's important later on\n",
        "    self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3} # token to integers\n",
        "    self.freq_threshold = freq_threshold # only recognize words with abs frequency >= freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    \"\"\"\n",
        "    Simple tokenizer: lowercase, remove punctuation, split by space.\n",
        "    \"\"\"\n",
        "    # This can be replaced with a library tokenizer\n",
        "    # I am writing a simple custom one for now, as I think sub-word tokenization is not needed for our use-case\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text) # NOTE: we might need to write special logic for handling numeric tokens\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split(\" \")\n",
        "\n",
        "  def build_vocab(self, sentence_list):\n",
        "    \"\"\"\n",
        "    Build a vocabulary of frequent words in sentence_list\n",
        "    \"\"\"\n",
        "    frequencies = {}\n",
        "    idx = 4 # we already have 4 words in our vocab (special tokens)\n",
        "\n",
        "    # count the frequencies of all the words\n",
        "    # NOTE: we might want to cap the vocabulary size. I am not doing it for now as I think it won't be needed for such a small dataset\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies: frequencies[word] = 1\n",
        "        else: frequencies[word] += 1\n",
        "\n",
        "    # assign an unique integer to words with abs frequency >= freq threshold\n",
        "    # for determinism, sort by frequency (descending) and then alphabetically (ascending)\n",
        "    sorted_words = sorted(frequencies.items(), key=lambda item: (-item[1], item[0]))\n",
        "    for word, count in sorted_words:\n",
        "      if count >= self.freq_threshold:\n",
        "        self.stoi[word] = idx\n",
        "        self.itos[idx] = word\n",
        "        idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    \"\"\"\n",
        "    Convert a text string into a list of integers\n",
        "    \"\"\"\n",
        "    # convert a text string into a list of integers\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "    return [self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"] for token in tokenized_text]\n",
        "\n",
        "  def decode(self, ids, skip_special_tokens=True):\n",
        "    \"\"\"\n",
        "    Reconstruct a sentence from integer vector\n",
        "    \"\"\"\n",
        "    if skip_special_tokens:\n",
        "        tokens = [self.itos.get(i, \"<unk>\") for i in ids if i not in [0, 1, 2]]  # skip <pad>, <start>, <end>\n",
        "    else:\n",
        "        tokens = [self.itos.get(i, \"<unk>\") for i in ids]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "  def get_stats(self):\n",
        "    return {\n",
        "        \"vocab_size\": len(self),\n",
        "        \"num_special_tokens\": 4,\n",
        "        \"num_words\": len(self) - 4\n",
        "    }\n",
        "\n",
        "  def save_vocabulary(self, filename):\n",
        "    \"\"\"\n",
        "    Saves vocabulary mappings to a file.\n",
        "    \"\"\"\n",
        "    # use pickle instead of json so that integer keys are saved as integers\n",
        "    with open(filename, \"wb\") as f:\n",
        "      pickle.dump({\n",
        "          \"stoi\": self.stoi,\n",
        "          \"itos\": self.itos,\n",
        "          \"freq_threshold\": self.freq_threshold\n",
        "      }, f)\n",
        "    print(f\"Vocabulary saved to {filename}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def load_vocabulary(filename):\n",
        "    \"\"\"\n",
        "    Loads a vocabulary from a file and returns a Vocabulary object\n",
        "    \"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "      data = pickle.load(f)\n",
        "\n",
        "    vocab = Vocabulary(freq_threshold=data[\"freq_threshold\"])\n",
        "    vocab.stoi = data[\"stoi\"]\n",
        "    vocab.itos = data[\"itos\"]\n",
        "\n",
        "    print(f\"Vocabulary loaded from {filename}\")\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed2cd8b-5ced-4c6d-a943-85a2e798381f",
      "metadata": {
        "id": "9ed2cd8b-5ced-4c6d-a943-85a2e798381f"
      },
      "outputs": [],
      "source": [
        "class CaptionDataset(Dataset):\n",
        "  def __init__(self, root_dir, images_list, captions_list, transform=None, freq_threshold=5, vocab=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    if not os.path.exists(root_dir):\n",
        "      raise FileNotFoundError(f\"Image directory not found: {root_dir}\")\n",
        "\n",
        "    self.imgs = images_list\n",
        "    self.captions = captions_list\n",
        "\n",
        "    # if a vocabulary is not passed in, build it\n",
        "    if vocab is not None: self.vocab = vocab\n",
        "    else:\n",
        "      self.vocab = Vocabulary(freq_threshold)\n",
        "      self.vocab.build_vocab(self.captions)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # get image caption + id (which in this case is the file name)\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "\n",
        "    # load image\n",
        "    img_path = os.path.join(self.root_dir, img_id)\n",
        "    if not os.path.exists(img_path):\n",
        "      raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "    # apply transformations\n",
        "    if self.transform is not None: img = self.transform(img)\n",
        "\n",
        "    # convert caption into a list of integers\n",
        "    numericalized_caption = [self.vocab.stoi[\"<start>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<end>\"]]\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot use the default pytorch data loader because the caption sentences are of different lengths and they cannot be stacked on top of each other. We need a custom function that will pad the batch to the length of the longest sentence in the batch."
      ],
      "metadata": {
        "id": "cjlv308lIns0"
      },
      "id": "cjlv308lIns0"
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    # a batch is a list of tuples (image, caption)\n",
        "    imgs = [item[0] for item in batch]\n",
        "    captions = [item[1] for item in batch]\n",
        "\n",
        "    # stack images (they are all already resized to the same size through transform)\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # pad captions. pad_sequence handles both padding and stacking\n",
        "    targets = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "    lengths = torch.tensor([len(c) for c in captions])\n",
        "\n",
        "    return imgs, targets, lengths"
      ],
      "metadata": {
        "id": "X3fLBFUQI7LJ"
      },
      "id": "X3fLBFUQI7LJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stratified train/val split to make sure that images are split between train/val to avoid data leakage\n",
        "df = pd.read_csv(cfg.CAPTIONS_FILE)\n",
        "unique_images = df[\"image\"].unique()\n",
        "np.random.shuffle(unique_images)\n",
        "train_img_count = int(cfg.TRAIN_SPLIT * len(unique_images))\n",
        "train_images = set(unique_images[:train_img_count])\n",
        "val_images = set(unique_images[train_img_count:])\n",
        "\n",
        "all_images = df[\"image\"].tolist()\n",
        "train_indices = [i for i, img in enumerate(all_images) if img in train_images]\n",
        "val_indices = [i for i, img in enumerate(all_images) if img in val_images]\n",
        "\n",
        "print(f\"Train images: {len(train_images)}, Val images: {len(val_images)}\")\n",
        "print(f\"Train captions: {len(train_indices)}, Val captions: {len(val_indices)}\")\n",
        "\n",
        "# build vocab only on training captions to avoid data leakage\n",
        "all_captions = df[\"caption\"].tolist()\n",
        "train_captions = [all_captions[i] for i in train_indices]\n",
        "vocab = Vocabulary(cfg.FREQ_THRESHOLD)\n",
        "vocab.build_vocab(train_captions)\n",
        "# vocab.save_vocabulary(cfg.VOCAB_PATH) # uncomment to save the vocabulary\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "train_dataset = CaptionDataset(\n",
        "    root_dir=cfg.IMAGE_DIR,\n",
        "    images_list=all_images,\n",
        "    captions_list=all_captions,\n",
        "    transform=train_transform,\n",
        "    freq_threshold=cfg.FREQ_THRESHOLD,\n",
        "    vocab=vocab\n",
        ")\n",
        "\n",
        "val_dataset = CaptionDataset(\n",
        "    root_dir=cfg.IMAGE_DIR,\n",
        "    images_list=all_images,\n",
        "    captions_list=all_captions,\n",
        "    transform=val_transform,\n",
        "    freq_threshold=cfg.FREQ_THRESHOLD,\n",
        "    vocab=vocab\n",
        ")\n",
        "\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(val_dataset, val_indices)\n",
        "\n",
        "pad_idx = vocab.stoi[\"<pad>\"]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True if cfg.NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=cfg.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True if cfg.NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_subset)}, Val samples: {len(val_subset)}\")"
      ],
      "metadata": {
        "id": "zl5rsbbfJ9_3"
      },
      "id": "zl5rsbbfJ9_3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "Now, it's time to implement the decoder."
      ],
      "metadata": {
        "id": "B3ejw4SPIO99"
      },
      "id": "B3ejw4SPIO99"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.5):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "\n",
        "    # embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0) # num of embeddings = vocab_size, length of embed vector = embed_size\n",
        "\n",
        "    # lstm layer\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=embed_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        batch_first=True,\n",
        "        dropout=dropout if num_layers > 1 else 0.0\n",
        "    )\n",
        "\n",
        "    # linear layer to convert lstm output to words score\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, features, captions, lengths):\n",
        "    \"\"\"\n",
        "    features: the image vectors (batch_size, embed_size)\n",
        "    captions: word indices (batch_size, seq_length)\n",
        "    lengths: the lengths of the sequences (batch_size)\n",
        "    \"\"\"\n",
        "    # embed the captions, ignore the <end> token\n",
        "    embeds = self.dropout(self.embedding(captions[:, :-1])) #dimensions: (batch_size, seq_length - 1, embed_size)\n",
        "\n",
        "    # turn image features into (batch_size, 1, embed_size) size\n",
        "    features = features.unsqueeze(1)\n",
        "\n",
        "    #treat the features vector as the first word in sequence\n",
        "    inputs = torch.cat((features, embeds), dim=1) #dimensions: (batch_size, seq_length, embed_size)\n",
        "\n",
        "    packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    hiddens, _ = self.lstm(packed)\n",
        "    hiddens, _ = pad_packed_sequence(hiddens, batch_first=True) # hiddens dimensions: (batch_size, sequence_length, hidden_size)\n",
        "    outputs = self.linear(hiddens) #dimensions: (batch_size, sequence_length, vocab_size)\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "zOg5pk8edL1o"
      },
      "id": "zOg5pk8edL1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoder = EfficientNetEncoder(embed_size)\n",
        "    self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "  def forward(self, images, captions, lengths):\n",
        "    features = self.encoder(images)\n",
        "    outputs = self.decoder(features, captions, lengths)\n",
        "    return outputs\n",
        "\n",
        "  def caption_image(self, image, vocabulary, max_length=50):\n",
        "    \"\"\"\n",
        "    image: input image tensor (batch, channel, height, width)\n",
        "    vocabulary: the Vocabulary object\n",
        "    max_length: the maximum length of the caption output\n",
        "    \"\"\"\n",
        "    self.eval()\n",
        "    result_caption = []\n",
        "    with torch.no_grad():\n",
        "      # get image features\n",
        "      x = self.encoder(image) # dim: (1, embed_size)\n",
        "      x = x.unsqueeze(1) # dim: (1, 1, embed_size)\n",
        "\n",
        "      # Initial LSTM states\n",
        "      states = None\n",
        "\n",
        "      # run the image through the LSTM\n",
        "      hiddens, states = self.decoder.lstm(x, states) # hiddens dim: (1, 1, hidden_size)\n",
        "\n",
        "      # start the loop with the <start> token\n",
        "      start_token_idx = vocabulary.stoi[\"<start>\"]\n",
        "\n",
        "      next_word_input = torch.tensor([[start_token_idx]]).to(image.device)\n",
        "\n",
        "      for _ in range(max_length):\n",
        "        # embed the current word input\n",
        "        embeds = self.decoder.embedding(next_word_input) # dim: (1, 1, embed_size)\n",
        "\n",
        "        # run lstm, feed the states from the previous step back in\n",
        "        hiddens, states = self.decoder.lstm(embeds, states) # dim: (1, 1, hidden_size)\n",
        "\n",
        "        # calculate the output probabilities\n",
        "        output = self.decoder.linear(hiddens[:, -1, :]) # dim: (1, vocab_size)\n",
        "\n",
        "        # pick the word with the highest probability NOTE: this can be changed\n",
        "        predicted = output.argmax(dim=1) # dim: (1)\n",
        "        item = predicted.item()\n",
        "\n",
        "        if item == vocabulary.stoi[\"<end>\"]: break # if predicted <end> token, stop\n",
        "\n",
        "        word = vocabulary.itos[item]\n",
        "        if word not in [\"<start>\", \"<pad>\"]:\n",
        "          result_caption.append(word)\n",
        "\n",
        "        next_word_input = predicted.unsqueeze(1) # dim: (1, 1)\n",
        "\n",
        "      return result_caption\n"
      ],
      "metadata": {
        "id": "_mjX1AQPeG7C"
      },
      "id": "_mjX1AQPeG7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHAIEQNNvbJs"
      },
      "id": "MHAIEQNNvbJs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}